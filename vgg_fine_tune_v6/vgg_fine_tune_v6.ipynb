{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werlang/emolearn-ml-model/blob/main/vgg_fine_tune_v6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "MoMicotOZ_pH",
        "outputId": "e96dcca8-e833-434b-fc39-9a9d7ea676dd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from keras.engine import  Model\n",
        "from keras.layers import Flatten, Dense, Input\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import TimeDistributed, GRU, LSTM, Dropout, Conv2D, BatchNormalization, MaxPooling2D\n",
        "from keras.utils import Sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, Callback, ModelCheckpoint\n",
        "from keras.metrics import AUC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, cv2\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "from keras import backend as K\n",
        "import math\n",
        "\n",
        "def start_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    !pip install keras_vggface\n",
        "\n",
        "\n",
        "def extract_data():\n",
        "    !mkdir features\n",
        "\n",
        "    # aligned faces extracted from openface\n",
        "    print(\"COPYING TRAIN SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/new_features_daisee/Train.zip\" -d features \n",
        "    print(\"COPYING TEST SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/new_features_daisee/Test.zip\" -d features \n",
        "    print(\"COPYING VALIDATION SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/new_features_daisee/Validation.zip\" -d features \n",
        "    print(\"COPYING LABELS...\")\n",
        "    !cp -r \"drive/My Drive/Doutorado/Implementação/new_features_daisee/labels\" ./\n",
        "    print(\"DONE\")\n",
        "\n",
        "\n",
        "def set_globals():\n",
        "    labels_path = \"labels\"\n",
        "    features_path = \"features\"\n",
        "    drive_save_path = 'drive/My Drive/1NOSYNC/DT/checkpoint'\n",
        "    batch_size = 10\n",
        "    labels = ['Test', 'Train', 'Validation']\n",
        "    time_frames = 10\n",
        "\n",
        "    return labels_path, features_path, drive_save_path, batch_size, labels, time_frames\n",
        "\n",
        "\n",
        "class Generator_V(Sequence):\n",
        "    def __init__(self, split, batch_size, frames, **kw):\n",
        "        self.batch_size = batch_size \n",
        "        self.split = split\n",
        "\n",
        "        Y = np.load(\"{}/{}.npy\".format(labels_path, split))\n",
        "        Y_gen = to_categorical(Y, 2)\n",
        "\n",
        "        self.labels, self.videos = [], []\n",
        "        interval = 2\n",
        "        stride = 0.5\n",
        "        fps = 15\n",
        "        skip = int(round(interval * fps / frames, 0))\n",
        "\n",
        "        for Yi in range(len(Y_gen)):\n",
        "            dir_path = \"{}/{}/{}\".format(features_path, split, Yi)\n",
        "            files = []\n",
        "            for r, d, f in os.walk(dir_path):\n",
        "                for i in f:\n",
        "                    files.append(\"{}/{}\".format(r, i))\n",
        "\n",
        "            for i in range(0, (len(files) - frames*skip) // skip + 1, np.max([1, int(stride * frames)])):\n",
        "                temp = []\n",
        "                for j in range(i*skip, (i+frames)*skip, skip):\n",
        "                    temp.append(files[j])\n",
        "                self.videos.append(temp)\n",
        "                self.labels.append(Y_gen[Yi])\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.videos) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.videos[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        videos = []\n",
        "        for video in batch_x:\n",
        "            images = []\n",
        "            for name in video:\n",
        "                images.append(cv2.imread(name))\n",
        "            videos.append(np.array(images)/255)\n",
        "\n",
        "        videos = np.array(videos)\n",
        "        label = np.array(batch_y)\n",
        "\n",
        "        return videos, label\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    from keras_vggface.vggface import VGGFace\n",
        "    \n",
        "    ########### HAVE TO SET IF THE MODEL IS TO BE LOADED FROM A CHECKPOINT OR TRAINED FROM SCRATCH ###########\n",
        "    loadModel = False\n",
        "\n",
        "    ########### IF BUILT, MUST DEFINE A NAME TO APPEND TO DIRECTORY NAME ###############\n",
        "    ident_name = 'fine-tune-images-vgg-striding-generator'\n",
        "    epoch = 0\n",
        "\n",
        "    ########### IF LOADED, MUST DEFINE DIR NAME AND STARTING EPOCH ############\n",
        "    if loadModel == True:\n",
        "        dir_name = '2020-6-21-2-50-38-fine-tune-images-vgg-resampled'\n",
        "        epoch = 2\n",
        "\n",
        "\n",
        "    #defining the model\n",
        "    vgg = VGGFace(\n",
        "        include_top=False,\n",
        "        input_shape=(224, 224, 3),\n",
        "        # model='resnet50',\n",
        "        weights = 'vggface')\n",
        "\n",
        "    for layer in vgg.layers[:-4]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # for training with vgg architecture\n",
        "    last_layer = vgg.get_layer('pool5').output\n",
        "    # for training resnet\n",
        "    # last_layer = vgg.get_layer('avg_pool').output\n",
        "    x = Flatten(name='flatten')(last_layer)\n",
        "\n",
        "    vgg_model = Model(vgg.input, x)\n",
        "\n",
        "    # print(\"Model 2d:\")\n",
        "    # for layer in vgg_model.layers:\n",
        "    #     print(\"{} {}\".format(layer.output.shape.as_list(), layer.trainable))\n",
        "\n",
        "    # print(vgg_model.summary())\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(TimeDistributed(vgg_model, input_shape=(time_frames, 224, 224, 3)))\n",
        "    model.add(GRU(4,return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    # model.add(LSTM(32, return_sequences=False))\n",
        "    # model.add(Dropout(0.2))\n",
        "\n",
        "    # model.add(Dense(16, activation='relu'))\n",
        "    # model.add(Dense(8, activation='relu'))\n",
        "\n",
        "    model.add(Dense(4, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(2, activation='softmax', name='classifier'))\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        # optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True), \n",
        "        optimizer = Adam(lr=1e-4),\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    #showing the model\n",
        "\n",
        "    # print(\"GRU:\")\n",
        "    # for layer in model.layers:\n",
        "    #     print(\"{} {}\".format(layer.output.shape.as_list(), layer.trainable))\n",
        "\n",
        "    # model.summary()\n",
        "\n",
        "    if loadModel == False:\n",
        "        t = datetime.datetime.now()\n",
        "        prefix = str(t.year) +'-'+ str(t.month) +'-'+ str(t.day) +'-'+ str(t.hour) +'-'+ str(t.minute) +'-'+ str(t.second)\n",
        "        save_dir = \"{}/{}-{}\".format(drive_save_path, prefix, ident_name)\n",
        "        os.mkdir(save_dir)\n",
        "    else:\n",
        "        file_name = \"{:03d}.h5\".format(epoch)\n",
        "        save_dir = \"{}/{}\".format(drive_save_path, dir_name)\n",
        "        print(\"Loading model from {}/{}.\".format(save_dir, file_name))\n",
        "        model = load_model(\"{}/{}\".format(save_dir, file_name))\n",
        "\n",
        "    return model, save_dir, epoch\n",
        "\n",
        "\n",
        "def set_callbacks():\n",
        "    #callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath = save_dir + '/{epoch:03d}.h5', \n",
        "        monitor = 'val_accuracy', \n",
        "        verbose=1, \n",
        "        save_best_only=True,\n",
        "        mode='max')\n",
        "\n",
        "    tensorboard = TensorBoard(\n",
        "    \tlog_dir         = \"{}/logs\".format(save_dir),\n",
        "    \thistogram_freq  = 0,\n",
        "    \twrite_graph     = True,\n",
        "    \twrite_grads     = False,\n",
        "    \twrite_images    = True)\n",
        "\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor \t= 'val_loss',\n",
        "        patience \t= 10,\n",
        "        mode \t\t= 'min',\n",
        "        restore_best_weights = True,\n",
        "        verbose     = 1,\n",
        "        min_delta   = 0.001)\n",
        "\n",
        "    reduce_lr_plateau = ReduceLROnPlateau(\n",
        "        monitor \t= 'val_loss',\n",
        "        factor\t\t= 0.1,\n",
        "        patience\t= 5,\n",
        "        mode \t\t= 'min',\n",
        "        min_lr\t\t= 1e-8,\n",
        "        verbose     = 1)\n",
        "\n",
        "    return [checkpoint, early_stop, reduce_lr_plateau]\n",
        "\n",
        "\n",
        "def build_generators():\n",
        "    #build the data generators\n",
        "    print(\"Building generators...\")\n",
        "    gen_train = Generator_V('Train', batch_size, time_frames)\n",
        "    gen_val = Generator_V('Validation', batch_size, time_frames)\n",
        "    print(\"DONE\")\n",
        "\n",
        "    return gen_train, gen_val\n",
        "\n",
        "\n",
        "def fit_model():\n",
        "    #calculate weights based on train set distribution\n",
        "    label_array = np.load(\"{}/Train.npy\".format(labels_path))\n",
        "    class_ratio = np.sum(label_array) / len(label_array)\n",
        "    weights = {0: class_ratio, 1: 1 - class_ratio}\n",
        "\n",
        "    def run():\n",
        "        #run the model\n",
        "        model.fit_generator(\n",
        "            gen_train,\n",
        "            epochs=1000,\n",
        "            validation_data=gen_val,\n",
        "            class_weight=weights,\n",
        "            callbacks=callbacks,\n",
        "            initial_epoch = epoch)\n",
        "        \n",
        "    run()        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "51ktnwPk3qjT",
        "outputId": "d512e0f0-7640-4e12-c0b4-8dfa9f21655f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# start_colab()\n",
        "# extract_data()\n",
        "labels_path, features_path, drive_save_path, batch_size, labels, time_frames = set_globals()\n",
        "model, save_dir, epoch = build_model()\n",
        "# callbacks = set_callbacks()\n",
        "# gen_train, gen_val = build_generators()\n",
        "# fit_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
