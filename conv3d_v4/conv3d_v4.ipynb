{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werlang/emolearn-ml-model/blob/main/conv3d_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoMicotOZ_pH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from keras.engine import  Model\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import TimeDistributed, GRU, LSTM, Dropout, Conv2D, Conv3D, BatchNormalization, MaxPooling2D, MaxPooling3D, Flatten, Dense, Input, Add, Activation, AveragePooling3D, Bidirectional\n",
        "from keras.utils import Sequence, plot_model, to_categorical\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, Callback, ModelCheckpoint\n",
        "from keras.metrics import AUC\n",
        "from keras.regularizers import l2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, cv2\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "from keras import backend as K\n",
        "import math\n",
        "from IPython.display import Image\n",
        "from numba import cuda\n",
        "import matplotlib.pyplot as plt\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# sm = SMOTE(sampling_strategy=0.6666)\n",
        "# X, y = sm.fit_resample(X, y)\n",
        "\n",
        "def restart():\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "\n",
        "\n",
        "def start_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    !pip install Keras-Applications\n",
        "\n",
        "\n",
        "def extract_data():\n",
        "    !mkdir features\n",
        "\n",
        "    # aligned faces extracted from openface\n",
        "    print(\"COPYING TRAIN SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/daisee_aligned/Train_112.zip\" -d features \n",
        "    print(\"COPYING VALIDATION SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/daisee_aligned/Validation_112.zip\" -d features \n",
        "    print(\"COPYING TEST SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/daisee_aligned/Test_112.zip\" -d features \n",
        "    print(\"COPYING LABELS...\")\n",
        "    !cp -r \"drive/My Drive/Doutorado/Implementação/labels\" ./\n",
        "    print(\"DONE\")\n",
        "\n",
        "\n",
        "class Generator_V(Sequence):\n",
        "    def __init__(self, split, batch_size, frames, pre_proc):\n",
        "        self.batch_size = batch_size \n",
        "        self.split = split\n",
        "        self.labels, self.videos = [], []\n",
        "\n",
        "        print(\"Building {} generator...\".format(split))\n",
        "        csv_path = \"{}/{}Labels.csv\".format(labels_path, split)\n",
        "        csv = pd.read_csv(csv_path)\n",
        "        Y = to_categorical(np.array(csv[emotion])//2, 2)\n",
        "        clips = csv['ClipID']\n",
        "            \n",
        "        fps = 15\n",
        "        skip = int(round(interval * fps / frames, 0))\n",
        "\n",
        "        for Yi in range(len(clips)):\n",
        "            file_name = clips[Yi].split(\".\")[0]\n",
        "            subject = file_name[:6]\n",
        "            dir_path = \"{}/{}/{}/{}_aligned\".format(features_path, self.split, subject, file_name)\n",
        "            files = []\n",
        "            for i in range(1,350):\n",
        "                image_path = \"{}/frame_det_00_000{:03d}.jpg\".format(dir_path, i)\n",
        "                if os.path.isfile(image_path):\n",
        "                    files.append(image_path)\n",
        "            \n",
        "            # ignore more than 150 files\n",
        "            if len(files) != 150:\n",
        "                print(len(files), files)\n",
        "\n",
        "            # files = files[:150]\n",
        "            # if len(files) < 150:\n",
        "            #     for i in range(150 - len(files)):\n",
        "            #         files.append(files[-1])\n",
        "\n",
        "            for i in range(0, (len(files) - frames*skip) // skip + 1, np.max([1, int(stride * frames)])):\n",
        "                temp = []\n",
        "                for j in range(i*skip, (i+frames)*skip, skip):\n",
        "                    temp.append(files[j])\n",
        "                self.videos.append(temp)\n",
        "                self.labels.append(Y[Yi])\n",
        "\n",
        "        if pre_proc:\n",
        "            Y = np.array(self.labels)\n",
        "            # majority group ratio after undesampling\n",
        "            us_size = 0.8\n",
        "            while sum(Y[:,1]) / len(Y) > us_size:\n",
        "                n = np.random.randint(0,len(Y))\n",
        "                if Y[n,1] == 1:\n",
        "                    self.videos.pop(n)\n",
        "                    self.labels.pop(n)\n",
        "                    Y = np.array(self.labels)\n",
        "\n",
        "            # minority group ratio after oversampling\n",
        "            os_size = 0.4\n",
        "            while sum(Y[:,0]) / len(Y) < os_size:\n",
        "                n = np.random.randint(0,len(Y))\n",
        "                if Y[n,0] == 1:\n",
        "                    i = np.random.randint(0,len(Y))\n",
        "                    self.videos.insert(i, self.videos[n])\n",
        "                    self.labels.insert(i, self.labels[n])\n",
        "                    Y = np.array(self.labels)\n",
        "\n",
        "\n",
        "        print(\"0: {} ({:.2f}), 1: {} ({:.2f})\".format(sum(Y[:,0]), sum(Y[:,0]) / len(Y) * 100, sum(Y[:,1]), sum(Y[:,1]) / len(Y) * 100))\n",
        "        np.save(\"{}/{}.npy\".format(labels_path, split), self.labels)\n",
        "        print(\"Done\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.videos) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.videos[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        # print(batch_x)\n",
        "        videos = []\n",
        "        for video in batch_x:\n",
        "            images = []\n",
        "            for name in video:\n",
        "                img = cv2.imread(name, 0)\n",
        "                # img = img[:,:,0].reshape((img.shape[0], img.shape[1], 1))\n",
        "                images.append(img)\n",
        "            videos.append(np.array(images)/255)\n",
        "            \n",
        "        videos = np.array(videos)\n",
        "        label = np.array(batch_y)\n",
        "\n",
        "        return videos, label\n",
        "\n",
        "\n",
        "def avgacc(y_true, y_pred):\n",
        "    def get_class_acc(id):\n",
        "        class_id_true = K.argmax(y_true, axis=-1)\n",
        "        class_id_pred = K.argmax(y_pred, axis=-1)\n",
        "        # Replace class_id_preds with class_id_true for recall here\n",
        "        accuracy_mask = K.cast(K.equal(class_id_pred, id), 'int32')\n",
        "        class_acc_tensor = K.cast(K.equal(class_id_true, class_id_pred), 'int32') * accuracy_mask\n",
        "        class_acc = K.sum(class_acc_tensor) / K.maximum(K.sum(accuracy_mask), 1)\n",
        "        return class_acc\n",
        "\n",
        "    return sum([get_class_acc(0), get_class_acc(1)]) / 2\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    print(\"Building model...\")\n",
        "    ########### IF BUILT, MUST DEFINE A NAME TO APPEND TO DIRECTORY NAME ###############\n",
        "    global ident_name\n",
        "\n",
        "    ########### IF LOADED, MUST DEFINE DIR NAME AND STARTING EPOCH ############\n",
        "    global dir_name\n",
        "\n",
        "    def residual_block(input, output_size):\n",
        "        KR = None\n",
        "        input_size = input.shape[-1]\n",
        "        \n",
        "        s = 1 if input_size == output_size else 2\n",
        "        x = Conv3D(output_size//4, strides=s, kernel_regularizer=KR, kernel_size=1, padding='valid')(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(act_func)(x)\n",
        "\n",
        "        x = Conv3D(output_size//4, strides=1, kernel_regularizer=KR, kernel_size=3, padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(act_func)(x)\n",
        "\n",
        "        x = Conv3D(output_size, strides=1, kernel_regularizer=KR, kernel_size=1, padding='valid')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        if input_size == output_size:\n",
        "            shortcut = input\n",
        "        else:\n",
        "            shortcut = Conv3D(output_size, strides=2, kernel_regularizer=KR, kernel_size=1, padding='valid')(input)\n",
        "            shortcut = BatchNormalization()(shortcut)\n",
        "\n",
        "        x = Add()([x, shortcut])\n",
        "        x = Activation(act_func)(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    KR = l2(1e-2)\n",
        "    # KR = None\n",
        "    act_func = 'relu'\n",
        "    filters = 16\n",
        "\n",
        "    input = Input(shape=(time_frames,112,112,1))\n",
        "    x = Conv3D(filters, kernel_regularizer=KR, kernel_size=3)(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(act_func)(x)\n",
        "    x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "    \n",
        "    blocks = [3, 4, 6, 3]\n",
        "    for b in blocks:\n",
        "        filters = filters * 2\n",
        "        for i in range(b):\n",
        "            x = residual_block(x, filters)\n",
        "\n",
        "    x = AveragePooling3D(pool_size=(2, 2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(256, kernel_regularizer=KR, activation=act_func)(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Dense(128, kernel_regularizer=KR, activation=act_func)(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Dense(64, kernel_regularizer=KR, activation=act_func)(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Dense(2, activation='softmax')(x)\n",
        "    model = Model(input, x)\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        # optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9), \n",
        "        optimizer = Adam(lr=1e-3),\n",
        "        metrics=[avgacc,'accuracy'])\n",
        "\n",
        "    plot_model(model, show_layer_names=False, show_shapes=True)\n",
        "\n",
        "    loadModel = False if epoch == 0 else True\n",
        "\n",
        "    if not loadModel:\n",
        "        t = datetime.datetime.now()\n",
        "        prefix = str(t.year) +'-'+ str(t.month) +'-'+ str(t.day) +'-'+ str(t.hour) +'-'+ str(t.minute) +'-'+ str(t.second)\n",
        "        save_dir = \"{}/{}-{}\".format(drive_save_path, prefix, ident_name)\n",
        "        os.mkdir(save_dir)\n",
        "    else:\n",
        "        file_name = \"{:03d}.h5\".format(epoch)\n",
        "        save_dir = \"{}/{}\".format(drive_save_path, dir_name)\n",
        "        print(\"Loading model from {}/{}.\".format(save_dir, file_name))\n",
        "        model = load_model(\"{}/{}\".format(save_dir, file_name), custom_objects={'avgacc': avgacc})\n",
        "\n",
        "    return model, save_dir, epoch\n",
        "\n",
        "\n",
        "def set_callbacks():\n",
        "    #callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath = save_dir + '/{epoch:03d}.h5', \n",
        "        monitor = 'val_loss', \n",
        "        verbose=1, \n",
        "        save_best_only=True,\n",
        "    )\n",
        "\n",
        "    tensorboard = TensorBoard(\n",
        "    \tlog_dir         = \"{}/logs\".format(save_dir),\n",
        "    \thistogram_freq  = 0,\n",
        "    \twrite_graph     = True,\n",
        "    \twrite_grads     = False,\n",
        "    \twrite_images    = True\n",
        "    )\n",
        "\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor \t= 'val_loss',\n",
        "        patience \t= 10,\n",
        "        restore_best_weights = True,\n",
        "        verbose     = 1,\n",
        "        min_delta   = 1e-5\n",
        "    )\n",
        "\n",
        "    reduce_lr_plateau = ReduceLROnPlateau(\n",
        "        monitor \t= 'val_loss',\n",
        "        factor\t\t= 0.2,\n",
        "        patience\t= 5,\n",
        "        min_lr\t\t= 1e-6,\n",
        "        verbose     = 1\n",
        "    )\n",
        "\n",
        "    return [checkpoint, early_stop, reduce_lr_plateau]\n",
        "\n",
        "\n",
        "def fit_model(train_weights):\n",
        "    #calculate weights based on train set distribution\n",
        "    label_array = np.load(\"{}/Train.npy\".format(labels_path))\n",
        "    weights = {0: 1, 1: 1}\n",
        "    if train_weights != [1,1]:\n",
        "        weights = {0: train_weights[0], 1: train_weights[1]}\n",
        "        print(\"Train weights: {}\".format(weights))\n",
        "\n",
        "    def run():\n",
        "        #run the model\n",
        "        return model.fit(\n",
        "            gen_train,\n",
        "            epochs = 1000,\n",
        "            validation_data = gen_val,\n",
        "            # class_weight = weights,\n",
        "            callbacks = callbacks,\n",
        "            initial_epoch = epoch)\n",
        "        \n",
        "    hist = run()  \n",
        "\n",
        "    plt.plot(hist.history['loss'], '#0000ff', label=\"loss\")\n",
        "    plt.plot(hist.history['val_loss'], '#ff0000', label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(hist.history['loss'], '#0000ff', label=\"loss\")\n",
        "    plt.plot(hist.history['val_loss'], '#ff0000', label=\"val_loss\")\n",
        "    plt.axis([5, None, 0, None])\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(hist.history['avgacc'], '#0000ff', label=\"avgacc\")\n",
        "    plt.plot(hist.history['val_avgacc'], '#ff0000', label=\"val_avgacc\")\n",
        "    plt.plot(hist.history['accuracy'], '#0055aa', label=\"acc\")\n",
        "    plt.plot(hist.history['val_accuracy'], '#aa5500', label=\"val_acc\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def predict(**kw):\n",
        "    global gen_test\n",
        "    try:\n",
        "        gen_test\n",
        "    except:\n",
        "        gen_test = Generator_OF(\"Test\", batch_size, False)\n",
        "    \n",
        "    if (kw and kw['force']):\n",
        "        gen_test = Generator_OF(\"Test\", batch_size, False)\n",
        "\n",
        "    Y_true = np.load(\"{}/Test.npy\".format(labels_path)).argmax(axis=1)\n",
        "\n",
        "    print(\"Predicting...\")\n",
        "    Y_pred = (model.predict(gen_test, verbose=1)).argmax(axis=1)\n",
        "    print(\"\\nConfusion matrix:\")\n",
        "    cm = confusion_matrix(Y_true, Y_pred)\n",
        "    print(cm)\n",
        "\n",
        "    print(\"\\nEvaluating...\")\n",
        "    ev = model.evaluate(gen_test, verbose=1)\n",
        "    # print(\"Loss: {}, Acc: {}\".format(ev[0], ev[1]))\n",
        "\n",
        "    hits = [0,0]    \n",
        "    for i in range(len(Y_true)):\n",
        "        if (Y_true[i] == 0 and Y_pred[i] == 0):\n",
        "            hits[0] = hits[0] + 1\n",
        "        if (Y_true[i] == 1 and Y_pred[i] == 1):\n",
        "            hits[1] = hits[1] + 1\n",
        "\n",
        "    hits[0] = hits[0] / (len(Y_true) - sum(Y_true))\n",
        "    hits[1] = hits[1] / sum(Y_true)\n",
        "    print(\"\\nPer class accuracy:\\n0: {}\\n1: {}\".format(hits[0], hits[1]))\n",
        "    print(\"Average class accuracy: {}\".format(sum(hits) / len(hits)))\n",
        "    return gen_test\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "51ktnwPk3qjT",
        "outputId": "e15c22eb-8421-4e75-9944-4444eb7382d5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "emotion = \"Engagement\"\n",
        "labels_path = \"labels\"\n",
        "features_path = \"features\"\n",
        "drive_save_path = 'drive/My Drive/1NOSYNC/DT/checkpoint'\n",
        "ident_name = 'conv3d-true-resnet-112px-1c'\n",
        "dir_name = '2020-10-9-22-54-29-openface-daisee-avgacc'\n",
        "batch_size = 32\n",
        "time_frames = 50\n",
        "interval = 5\n",
        "stride = 1\n",
        "epoch = 0\n",
        "\n",
        "# restart()\n",
        "# start_colab()\n",
        "# extract_data()\n",
        "\n",
        "gen_train = Generator_V('Train', batch_size, time_frames, True)\n",
        "gen_val = Generator_V('Validation', batch_size, time_frames, False)\n",
        "model, save_dir, epoch = build_model()\n",
        "callbacks = set_callbacks()\n",
        "fit_model([1,1])\n",
        "\n",
        "# gen_test = Generator_V('Test', batch_size, time_frames, False)\n",
        "# gen_test = predict()\n",
        "\n",
        "# Image('model.png')\n",
        "# model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
