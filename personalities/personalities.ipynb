{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werlang/emolearn-ml-model/blob/main/personalities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwpi09Jo6O2S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence, plot_model, to_categorical\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import TimeDistributed, GRU, LSTM, Dropout, Conv1D, Conv2D, Conv3D, ConvLSTM2D, BatchNormalization, MaxPooling1D, MaxPooling2D, MaxPooling3D, GlobalAveragePooling2D, Flatten, Dense, Input, Add, Activation, AveragePooling3D, AveragePooling2D, ZeroPadding3D, Bidirectional, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Nadam\n",
        "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, Callback, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, cv2\n",
        "import datetime\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "from IPython.display import Image, display\n",
        "from numba import cuda\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler \n",
        "import functools\n",
        "\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# sm = SMOTE(sampling_strategy=0.6666)\n",
        "# X, y = sm.fit_resample(X, y)\n",
        "\n",
        "\n",
        "drive_save_path = 'drive/My Drive/1NOSYNC/DT/checkpoint'\n",
        "ident_name = 'personality'\n",
        "dir_name = '2021-12-9-2-44-8-personality'\n",
        "batch_size = 50\n",
        "time_frames = 20\n",
        "interval = 2\n",
        "stride = 1\n",
        "fold_step = 1\n",
        "n_folds = 10\n",
        "\n",
        "epoch = 0\n",
        "\n",
        "\n",
        "def restart():\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "\n",
        "\n",
        "def start_colab():\n",
        "    from google.colab import drive\n",
        "    drive._mount('/content/drive', force_remount=True)\n",
        "    !pip install Keras-Applications\n",
        "    # !pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "    !pip install keras-tcn\n",
        "\n",
        "\n",
        "def extract_data():\n",
        "    !mkdir p2m\n",
        "    !mkdir p2m/features\n",
        "\n",
        "    # aligned faces extracted from openface\n",
        "    print(\"COPYING ALIGNED FACES...\")\n",
        "    !unzip -n -q \"drive/My Drive/1NOSYNC/DT/p2m_dataset/224/p2m_faces.zip\" -d p2m/features\n",
        "    print(\"COPYING OPENFACE FEATURES...\")\n",
        "    !unzip -n -q \"drive/My Drive/1NOSYNC/DT/p2m_dataset/224/p2m_openface.zip\" -d p2m/features\n",
        "    print(\"COPYING LABELS...\")\n",
        "    !cp -r \"drive/My Drive/1NOSYNC/DT/p2m_dataset/224/labels.csv\" p2m\n",
        "    !cp -r \"drive/My Drive/1NOSYNC/DT/p2m_dataset/224/personalities.csv\" p2m\n",
        "\n",
        "    print(\"P2M DONE\")\n",
        "\n",
        "\n",
        "def create_folds_p2m():\n",
        "    num_classes = 2\n",
        "    label_window = time_frames\n",
        "    print(\"CREATING FOLDS...\")\n",
        "    subs = {}\n",
        "\n",
        "    csv = pd.read_csv('p2m/labels.csv')\n",
        "    Y = np.array(csv.iloc[:,3])\n",
        "    subjects = np.array(csv['SUBJECT'])\n",
        "    clips = np.array(csv['CLIP'])\n",
        "    source_frames = np.array(csv['SOURCE_FRAMES'])\n",
        "\n",
        "    Y = to_categorical(Y, num_classes)    \n",
        "\n",
        "    for i in range(len(subjects)):\n",
        "        if not subjects[i] in subs:\n",
        "            subs[subjects[i]] = [0 for x in range(num_classes)]\n",
        "        subs[subjects[i]] = np.add(subs[subjects[i]], Y[i])\n",
        "\n",
        "    # total = np.sum(Y, axis=0)\n",
        "    # print(total, total / len(Y))\n",
        "    # print(subs)\n",
        "\n",
        "    folds = []\n",
        "    fold_names = []\n",
        "    for i in subs:\n",
        "        if len(folds) < n_folds + 1:\n",
        "            folds.append(subs[i])\n",
        "            fold_names.append([i])\n",
        "        else:\n",
        "            summed = np.sum(folds, axis=1)\n",
        "            index = list(summed).index(min(summed))\n",
        "            folds[index] = np.add(folds[index], subs[i])\n",
        "            fold_names[index].append(i)\n",
        "\n",
        "    # print(folds)\n",
        "    # print(fold_names)\n",
        "    # print(np.sum(folds, axis=1))\n",
        "\n",
        "    # print(\"GETTING FOLDS DATA...\")\n",
        "    # build frames array\n",
        "    videos, labels, opface, xlabel, pers = [], [], [], [], []\n",
        "    for i in range(len(fold_names)):\n",
        "        videos.append([])\n",
        "        opface.append([])\n",
        "        labels.append([])\n",
        "        xlabel.append([])\n",
        "        pers.append([])\n",
        "\n",
        "    fps = 30\n",
        "    skip = int(round(interval * fps / time_frames, 0))\n",
        "\n",
        "    # pick personalities csv and scale\n",
        "    personality_csv = pd.read_csv('p2m/personalities.csv')\n",
        "    personality_csv = np.array(personality_csv)[:,1:]\n",
        "    min_max_scaler = MinMaxScaler()\n",
        "    personality_csv = min_max_scaler.fit_transform(personality_csv)\n",
        "\n",
        "    for Yi in range(len(Y)):\n",
        "        print(\"\\rCLIP {}/{}\".format(Yi, len(Y)), end=\"\")\n",
        "        clip = clips[Yi]\n",
        "        start_frame = source_frames[Yi].split(\"-\")[0]\n",
        "        subject = subjects[Yi]\n",
        "        subject_personality = personality_csv[int(subject)]\n",
        "\n",
        "        for f in range(n_folds + 1):\n",
        "            if int(subject) in fold_names[f]:\n",
        "                files, filesof = [], []\n",
        "                \n",
        "                of_path = \"p2m/features/{:02}.csv\".format(subject)\n",
        "\n",
        "                for i in range(60):\n",
        "                    image_path = \"p2m/features/faces/{:02}.{:03}.{:02}.jpg\".format(subject, clip, i)\n",
        "                    \n",
        "                    if os.path.isfile(image_path) and os.path.isfile(of_path):\n",
        "                        files.append(image_path)\n",
        "                        filesof.append(\"{}|{}\".format(of_path, int(start_frame) + i))\n",
        "\n",
        "                # append only the parts relative to the video section\n",
        "                last = (len(files) - time_frames*skip) // skip\n",
        "                rg = range(0, last + 1, np.max([1, int(stride * time_frames)]))\n",
        "                for i in rg:\n",
        "                    temp, tpof = [], []\n",
        "                    for j in range(i*skip, (i+time_frames)*skip, skip):\n",
        "                        temp.append(files[j])\n",
        "                        tpof.append(filesof[j])\n",
        "                    videos[f].append(temp)\n",
        "                    opface[f].append(tpof)\n",
        "                    labels[f].append(Y[Yi])\n",
        "\n",
        "                    # get previous clip labels from same subject\n",
        "                    xl = []\n",
        "                    for l in range(Yi - 1, 0, -1):\n",
        "                        sub2 = subjects[l]\n",
        "                        if subject == sub2:\n",
        "                            xl.append(Y[l])\n",
        "                        if len(xl) == label_window:\n",
        "                            break\n",
        "                        \n",
        "                    xl = [(xl[0] if len(xl) > 0 else np.array([0., 1.])) for x in range(label_window - len(xl))] + xl\n",
        "                    xlabel[f].append(xl)\n",
        "\n",
        "                    xp = np.array([subject_personality for x in range(label_window)])\n",
        "                    pers[f].append(xp)\n",
        "\n",
        "\n",
        "    print(\"\\nDONE\")\n",
        "    return videos, opface, xlabel, pers, labels\n",
        "\n",
        "\n",
        "def save_folds(folds):\n",
        "    !mkdir folds\n",
        "\n",
        "    first_only = True\n",
        "\n",
        "    folds_copy = folds.copy()\n",
        "\n",
        "    if len(folds) == 1:\n",
        "        test_X = folds_copy[0][0].pop(0)\n",
        "        test_XO = folds_copy[0][1].pop(0)\n",
        "        test_XL = folds_copy[0][2].pop(0)\n",
        "        test_XP = folds_copy[0][3].pop(0)\n",
        "        test_Y = folds_copy[0][4].pop(0)\n",
        "\n",
        "        videos = folds_copy[0][0]\n",
        "        opface = folds_copy[0][1]\n",
        "        xlabel = folds_copy[0][2]\n",
        "        person = folds_copy[0][3]\n",
        "        labels = folds_copy[0][4]\n",
        "\n",
        "    test_Y = np.array(test_Y)\n",
        "    print(test_Y.shape)\n",
        "    np.save(\"folds/test_Y.npy\", test_Y)\n",
        "    \n",
        "    test_X = np.array(test_X)\n",
        "    print(test_X.shape)\n",
        "    np.save(\"folds/test_XV.npy\", test_X)\n",
        "\n",
        "    test_XO = np.array(test_XO)\n",
        "    print(test_XO.shape)\n",
        "    np.save(\"folds/test_XO.npy\", test_XO)\n",
        "\n",
        "    test_XL = np.array(test_XL)\n",
        "    print(test_XL.shape)\n",
        "    np.save(\"folds/test_XL.npy\", test_XL)\n",
        "\n",
        "    test_XP = np.array(test_XP)\n",
        "    print(test_XP.shape)\n",
        "    np.save(\"folds/test_XP.npy\", test_XP)\n",
        "\n",
        "    for i in range(n_folds):\n",
        "        print(\"SAVING FOLD {}...\".format(i+1))\n",
        "        train_X = videos.copy()\n",
        "        train_XO = opface.copy()\n",
        "        train_XL = xlabel.copy()\n",
        "        train_XP = person.copy()\n",
        "        train_Y = labels.copy()\n",
        "\n",
        "        val_X = train_X.pop(i)\n",
        "        val_XO = train_XO.pop(i)\n",
        "        val_XL = train_XL.pop(i)\n",
        "        val_XP = train_XP.pop(i)\n",
        "        val_Y = train_Y.pop(i)\n",
        "\n",
        "        train_X = np.concatenate(train_X)\n",
        "        train_XO = np.concatenate(train_XO)\n",
        "        train_XL = np.concatenate(train_XL)\n",
        "        train_XP = np.concatenate(train_XP)\n",
        "        train_Y = np.concatenate(train_Y)\n",
        "\n",
        "        # create index array for resample\n",
        "        I = np.array([x for x in range(len(train_X))]).reshape(-1,1)\n",
        "\n",
        "        # I, train_Y = ros.fit_resample(I, train_Y)\n",
        "        # I, train_Y = rus.fit_resample(I, train_Y)\n",
        "        # train_Y = to_categorical(train_Y, len(original))\n",
        "\n",
        "        # copy resampled elements from original arrays according to resampling indexes\n",
        "        ntx, ntxo, ntxl, ntxp = [], [], [], []\n",
        "        for x in I:\n",
        "            ntx.append(train_X[x[0]])\n",
        "            ntxo.append(train_XO[x[0]])\n",
        "            ntxl.append(train_XL[x[0]])\n",
        "            ntxp.append(train_XP[x[0]])\n",
        "        train_X = ntx\n",
        "        train_XO = ntxo\n",
        "        train_XL = ntxl\n",
        "        train_XP = ntxp\n",
        "\n",
        "        # save folds to file\n",
        "        train_Y = np.array(train_Y)\n",
        "        print(train_Y.shape)\n",
        "        np.save(\"folds/fold_{}_train_Y.npy\".format(i), train_Y)\n",
        "        \n",
        "        train_X = np.array(train_X)\n",
        "        print(train_X.shape)\n",
        "        np.save(\"folds/fold_{}_train_XV.npy\".format(i), train_X)\n",
        "\n",
        "        train_XO = np.array(train_XO)\n",
        "        print(train_XO.shape)\n",
        "        np.save(\"folds/fold_{}_train_XO.npy\".format(i), train_XO)\n",
        "\n",
        "        train_XL = np.array(train_XL)\n",
        "        print(train_XL.shape)\n",
        "        np.save(\"folds/fold_{}_train_XL.npy\".format(i), train_XL)\n",
        "        \n",
        "        train_XP = np.array(train_XP)\n",
        "        print(train_XP.shape)\n",
        "        np.save(\"folds/fold_{}_train_XP.npy\".format(i), train_XP)\n",
        "\n",
        "        val_Y = np.array(val_Y)\n",
        "        print(val_Y.shape)\n",
        "        np.save(\"folds/fold_{}_validation_Y.npy\".format(i), val_Y)\n",
        "        \n",
        "        val_X = np.array(val_X)\n",
        "        print(val_X.shape)\n",
        "        np.save(\"folds/fold_{}_validation_XV.npy\".format(i), val_X)\n",
        "\n",
        "        val_XO = np.array(val_XO)\n",
        "        print(val_XO.shape)\n",
        "        np.save(\"folds/fold_{}_validation_XO.npy\".format(i), val_XO)\n",
        "\n",
        "        val_XL = np.array(val_XL)\n",
        "        print(val_XL.shape)\n",
        "        np.save(\"folds/fold_{}_validation_XL.npy\".format(i), val_XL)\n",
        "\n",
        "        val_XP = np.array(val_XP)\n",
        "        print(val_XP.shape)\n",
        "        np.save(\"folds/fold_{}_validation_XP.npy\".format(i), val_XP)\n",
        "\n",
        "        if first_only:\n",
        "            break\n",
        "\n",
        "\n",
        "class Generator_V(Sequence):\n",
        "    def __init__(self, gen_split, batch_size, frames):\n",
        "        print(\"BUILDING GENERATOR...\")\n",
        "        self.batch_size = batch_size \n",
        "        self.labels, self.videos, = [], []\n",
        "        self.loadedOF = {}\n",
        "\n",
        "        file_prefix = \"\" if gen_split == \"Test\" else \"fold_{}_\".format(fold_step-1)\n",
        "        self.videos = np.load(\"folds/{}{}_XV.npy\".format(file_prefix, gen_split.lower()))\n",
        "        self.opface = np.load(\"folds/{}{}_XO.npy\".format(file_prefix, gen_split.lower()))\n",
        "        self.xlabel = np.load(\"folds/{}{}_XL.npy\".format(file_prefix, gen_split.lower()))\n",
        "        self.person = np.load(\"folds/{}{}_XP.npy\".format(file_prefix, gen_split.lower()))\n",
        "        self.labels = np.load(\"folds/{}{}_Y.npy\".format(file_prefix, gen_split.lower()))\n",
        "\n",
        "        # self.videos, self.labels = shuffle(self.videos, self.labels, random_state=0)\n",
        "        print(\"Done\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.videos) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_xv = self.videos[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_xo = self.opface[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_xl = self.xlabel[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_xp = self.person[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        videos, opface = [], []\n",
        "        for video in batch_xv:\n",
        "            images = []\n",
        "            for name in video:\n",
        "                img = cv2.imread(name)\n",
        "                img = cv2.resize(img, (224, 224))\n",
        "                images.append(img/255)\n",
        "            videos.append(np.array(images))\n",
        "\n",
        "        for video in batch_xo:\n",
        "            images = []\n",
        "\n",
        "            of_path = video[0].split(\"|\")[0]\n",
        "            if not of_path in self.loadedOF:\n",
        "                csv_of = pd.read_csv(of_path)\n",
        "                rows = csv_of[[' gaze_0_x',' gaze_0_y',' gaze_0_z',' gaze_1_x',' gaze_1_y',' gaze_1_z',' gaze_angle_x',' gaze_angle_y',' pose_Tx',' pose_Ty',' pose_Tz',' pose_Rx',' pose_Ry',' pose_Rz',' AU01_r',' AU02_r',' AU04_r',' AU05_r',' AU06_r',' AU07_r',' AU09_r',' AU10_r',' AU12_r',' AU14_r',' AU15_r',' AU17_r',' AU20_r',' AU23_r',' AU25_r',' AU26_r',' AU45_r',' AU01_c',' AU02_c',' AU04_c',' AU05_c',' AU06_c',' AU07_c',' AU09_c',' AU10_c',' AU12_c',' AU14_c',' AU15_c',' AU17_c',' AU20_c',' AU23_c',' AU25_c',' AU26_c',' AU28_c',' AU45_c']]\n",
        "                min_max_scaler = MinMaxScaler()\n",
        "                self.loadedOF[of_path] = min_max_scaler.fit_transform(rows)\n",
        "\n",
        "            rows = self.loadedOF[of_path]\n",
        "\n",
        "            for name in video:\n",
        "                i = int(name.split(\"|\")[1])\n",
        "                try:\n",
        "                    img = rows[i]\n",
        "                except:\n",
        "                    print(\"ERROR\", i, len(rows))\n",
        "                images.append(img)\n",
        "\n",
        "            opface.append(np.array(images))\n",
        "\n",
        "        videos = np.array(videos)\n",
        "        opface = np.array(opface)\n",
        "        xlabel = np.array(batch_xl)\n",
        "        person = np.array(batch_xp)\n",
        "        label = np.array(batch_y)\n",
        "\n",
        "        # return [videos, opface, xlabel, person], label\n",
        "        # return [videos, opface, xlabel], label\n",
        "        return [videos, opface], label\n",
        "\n",
        "\n",
        "def build_model(epoch=0, print_model=False):\n",
        "    from tcn import TCN, tcn_full_summary\n",
        "\n",
        "    img_size = 224\n",
        "    of_features = 49\n",
        "\n",
        "    def model_convlstm(input):\n",
        "        filters = 256\n",
        "\n",
        "        x = ConvLSTM2D(filters, kernel_size=3, strides=1, padding='same', kernel_regularizer=l2(5e-4), recurrent_regularizer=l2(1e-6), return_sequences=True)(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        x = ConvLSTM2D(filters, kernel_size=3, strides=1, padding='same', kernel_regularizer=l2(5e-4), recurrent_regularizer=l2(1e-6), return_sequences=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        x = MaxPooling2D(pool_size=3, strides=2, padding='valid')(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def model_tcn(input):\n",
        "        filters = 512\n",
        "\n",
        "        x = TCN(filters)(input)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def model_inception(input):\n",
        "        inception = tf.keras.applications.InceptionResNetV2(\n",
        "            input_shape = (img_size, img_size, 3),\n",
        "            weights = 'imagenet',\n",
        "            include_top = False\n",
        "        )\n",
        "\n",
        "        for layer in inception.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = TimeDistributed(inception)(input)\n",
        "\n",
        "        return x\n",
        "\n",
        "    print(\"Building model...\")\n",
        "    ########### IF BUILT, MUST DEFINE A NAME TO APPEND TO DIRECTORY NAME ###############\n",
        "    global ident_name\n",
        "\n",
        "    ########### IF LOADED, MUST DEFINE DIR NAME AND STARTING EPOCH ############\n",
        "    global dir_name\n",
        "\n",
        "    if not epoch:\n",
        "        input_v = Input(shape=(time_frames, img_size, img_size, 3))\n",
        "        input_o = Input(shape=(time_frames, of_features))\n",
        "        # input_l = Input(shape=(time_frames, 2))\n",
        "        # input_p = Input(shape=(time_frames, 5))\n",
        "\n",
        "        mv = model_inception(input_v)\n",
        "        mv = model_convlstm(mv)\n",
        "        mo = model_tcn(input_o)\n",
        "        # ml = model_tcn(input_l)\n",
        "        # mp = model_tcn(input_p)\n",
        "\n",
        "        # joint model\n",
        "        x = Concatenate()([mv, mo])\n",
        "        # x = Concatenate()([mv, mo, ml])\n",
        "        # x = Concatenate()([mv, mo, ml, mp])\n",
        "        \n",
        "        x = Dense(512, activation='relu')(x)\n",
        "        x = Dropout(0.8)(x)\n",
        "        x = Dense(512, activation='relu')(x)\n",
        "        x = Dropout(0.8)(x)\n",
        "        x = Dense(2, activation='softmax')(x)\n",
        "\n",
        "        model = Model([input_v, input_o], x)\n",
        "        # model = Model([input_v, input_o, input_l], x)\n",
        "        # model = Model([input_v, input_o, input_l, input_p], x)\n",
        "\n",
        "        model.compile(\n",
        "            loss='categorical_crossentropy',\n",
        "            optimizer = SGD(learning_rate=0.0001, momentum=0.9), \n",
        "            metrics=['accuracy'])\n",
        "    \n",
        "        t = datetime.datetime.now()\n",
        "        prefix = str(t.year) +'-'+ str(t.month) +'-'+ str(t.day) +'-'+ str(t.hour) +'-'+ str(t.minute) +'-'+ str(t.second)\n",
        "        save_dir = \"{}/{}-{}\".format(drive_save_path, prefix, ident_name)\n",
        "        os.mkdir(save_dir)\n",
        "    else:\n",
        "        file_name = \"{:03d}.h5\".format(epoch)\n",
        "        save_dir = \"{}/{}\".format(drive_save_path, dir_name)\n",
        "        print(\"Loading model from {}/{}.\".format(save_dir, file_name))\n",
        "        model = load_model(\"{}/{}\".format(save_dir, file_name), custom_objects={'TCN': TCN})\n",
        "\n",
        "    if print_model:\n",
        "        plot_model(model, show_layer_names=False, show_shapes=True, expand_nested=True)\n",
        "        # display(Image('model.png'))\n",
        "\n",
        "    return model, save_dir, epoch\n",
        "\n",
        "\n",
        "def set_callbacks():\n",
        "    #callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath = save_dir + '/{epoch:03d}.h5', \n",
        "        monitor = 'val_loss', \n",
        "        verbose=1, \n",
        "        save_best_only=True,\n",
        "    )\n",
        "\n",
        "    tensorboard = TensorBoard(\n",
        "    \tlog_dir         = \"{}/logs\".format(save_dir),\n",
        "    \thistogram_freq  = 0,\n",
        "    \twrite_graph     = True,\n",
        "    \twrite_grads     = False,\n",
        "    \twrite_images    = True\n",
        "    )\n",
        "\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor \t= 'val_loss',\n",
        "        patience \t= 20,\n",
        "        restore_best_weights = True,\n",
        "        verbose     = 1,\n",
        "        min_delta   = 1e-5\n",
        "    )\n",
        "\n",
        "    reduce_lr_plateau = ReduceLROnPlateau(\n",
        "        monitor \t= 'val_loss',\n",
        "        factor\t\t= 0.5,\n",
        "        patience\t= 10,\n",
        "        min_lr\t\t= 1e-6,\n",
        "        verbose     = 1\n",
        "    )\n",
        "\n",
        "    # return [checkpoint, tensorboard]\n",
        "    return [checkpoint, early_stop, reduce_lr_plateau, tensorboard]\n",
        "\n",
        "\n",
        "def fit_model(train_weights, epoch=0):\n",
        "    #calculate weights based on train set distribution\n",
        "    num_classes = len(train_weights)\n",
        "    if train_weights == [1 for x in range(num_classes)]:\n",
        "        weights = {x:1 for x in range(num_classes)}\n",
        "    else:\n",
        "        weights = {x: train_weights[x] for x in range(len(train_weights))}\n",
        "        print(\"Train weights: {}\".format(weights))\n",
        "\n",
        "    def run():\n",
        "        #run the model\n",
        "        return model.fit(\n",
        "            gen_train,\n",
        "            epochs = 1000,\n",
        "            validation_data = gen_val,\n",
        "            class_weight = weights,\n",
        "            callbacks = callbacks,\n",
        "            initial_epoch = epoch)\n",
        "        \n",
        "    hist = run()  \n",
        "\n",
        "    plot_hist(hist)\n",
        "\n",
        "    return hist\n",
        "\n",
        "\n",
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history['loss'], '#0000ff', label=\"loss\")\n",
        "    plt.plot(hist.history['val_loss'], '#ff0000', label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(hist.history['accuracy'], '#0000ff', label=\"acc\")\n",
        "    plt.plot(hist.history['val_accuracy'], '#ff0000', label=\"val_acc\")\n",
        "    # plt.plot(hist.history['accuracy'], '#0055aa', label=\"acc\")\n",
        "    # plt.plot(hist.history['val_accuracy'], '#aa5500', label=\"val_acc\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def predict(**kw):\n",
        "    global gen_test\n",
        "\n",
        "    Y_true = np.load(\"folds/test_Y.npy\")\n",
        "    num_classes = Y_true.shape[1]\n",
        "    Y_true = Y_true.argmax(axis=1)\n",
        "\n",
        "    print(\"Predicting...\")\n",
        "    Y_pred = model.predict(gen_test, verbose=1).argmax(axis=1)\n",
        "    print(\"\\nConfusion matrix:\")\n",
        "    cm = confusion_matrix(Y_true, Y_pred)\n",
        "    print(cm)\n",
        "\n",
        "    print(\"\\nEvaluating...\")\n",
        "    ev = model.evaluate(gen_test, verbose=1)\n",
        "    # print(\"Loss: {}, Acc: {}\".format(ev[0], ev[1]))\n",
        "\n",
        "    hits = [0 for x in range(num_classes)]\n",
        "    total = [0 for x in range(num_classes)]\n",
        "    for i in range(len(Y_true)):\n",
        "        for c in range(num_classes):\n",
        "            if (Y_true[i] == c and Y_pred[i] == c):\n",
        "                hits[c] = hits[c] + 1\n",
        "            if Y_true[i] == c:\n",
        "                total[c] = total[c] + 1\n",
        "\n",
        "    print(\"\\nPer class accuracy:\")\n",
        "    for c in range(num_classes):\n",
        "        hits[c] = hits[c] / total[c]\n",
        "        print(\"{}: {}\".format(c, hits[c]))\n",
        "\n",
        "    print(\"Average class accuracy: {}\".format(sum(hits) / len(hits)))\n",
        "\n",
        "    f1 = f1_score(Y_true, Y_pred, average=None)\n",
        "    print(\"F1 score: {}. Avg F1: {}\".format(f1, np.mean(f1)))\n",
        "\n",
        "    return gen_test\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QmXyo3TVhQpV",
        "outputId": "e1f4fae1-4965-4a1a-ab78-4f8647f97bea"
      },
      "outputs": [],
      "source": [
        "# restart()\n",
        "# start_colab()\n",
        "# extract_data()\n",
        "# fold_p2m = create_folds_p2m()\n",
        "# save_folds([fold_p2m])\n",
        "\n",
        "gen_train = Generator_V('Train', batch_size, time_frames)\n",
        "gen_val = Generator_V('Validation', batch_size, time_frames)\n",
        "gen_test = Generator_V('Test', batch_size, time_frames)\n",
        "\n",
        "model, save_dir, epoch = build_model(epoch, print_model=False)\n",
        "\n",
        "callbacks = set_callbacks()\n",
        "fit_model(list(len(gen_train.labels) / np.sum(gen_train.labels, axis=0)), epoch)\n",
        "gen_test = predict()\n",
        "\n",
        "# model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOwulxBLLSbIVb5DWtnbbEQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "personalities.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
