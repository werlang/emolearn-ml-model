{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werlang/emolearn-ml-model/blob/main/daisee_eng_merge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwpi09Jo6O2S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence, plot_model, to_categorical\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import TimeDistributed, GRU, LSTM, Dropout, Conv1D, Conv2D, Conv3D, ConvLSTM2D, BatchNormalization, MaxPooling1D, MaxPooling2D, MaxPooling3D, GlobalAveragePooling2D, Flatten, Dense, Input, Add, Activation, AveragePooling3D, AveragePooling2D, ZeroPadding3D, Bidirectional, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Nadam\n",
        "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, Callback, ModelCheckpoint\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, cv2\n",
        "import datetime\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "from IPython.display import Image, display\n",
        "from numba import cuda\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import RandomOverSampler \n",
        "import functools\n",
        "\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "# sm = SMOTE(sampling_strategy=0.6666)\n",
        "# X, y = sm.fit_resample(X, y)\n",
        "\n",
        "\n",
        "drive_save_path = 'drive/My Drive/1NOSYNC/DT/checkpoint'\n",
        "labels_path = 'labels'\n",
        "features_path = 'features'\n",
        "ident_name = 'daisee-eng-merge'\n",
        "dir_name = '2021-6-22-12-47-14-daisee-eng-merge'\n",
        "batch_size = 64\n",
        "time_frames = 20\n",
        "interval = 2\n",
        "stride = 1\n",
        "fold_step = 1\n",
        "n_folds = 5\n",
        "\n",
        "epoch = 0\n",
        "\n",
        "def restart():\n",
        "    cuda.select_device(0)\n",
        "    cuda.close()\n",
        "\n",
        "\n",
        "def start_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    !pip install Keras-Applications\n",
        "    # !pip install git+https://github.com/rcmalli/keras-vggface.git\n",
        "    !pip install keras-tcn\n",
        "\n",
        "\n",
        "def extract_data():\n",
        "    !mkdir features\n",
        "\n",
        "    print(\"COPYING TRAIN SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/1NOSYNC/DT/daisee_aligned/Train.zip\" -d features \n",
        "    print(\"COPYING VALIDATION SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/1NOSYNC/DT/daisee_aligned/Validation.zip\" -d features \n",
        "    print(\"COPYING TEST SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/1NOSYNC/DT/daisee_aligned/Test.zip\" -d features \n",
        "    print(\"COPYING OPENFACE FEATURES...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/daisee_openface.zip\" -d of_features\n",
        "    print(\"COPYING LABELS...\")\n",
        "    !cp -r \"drive/My Drive/Doutorado/Implementação/labels\" ./\n",
        "\n",
        "    print(\"DONE\")\n",
        "\n",
        "\n",
        "def create_folds():\n",
        "    num_classes = 2\n",
        "    print(\"CREATING FOLDS...\")\n",
        "    subs = {}\n",
        "\n",
        "    Y = []\n",
        "    clips = []\n",
        "    # just concatenate all splits\n",
        "    for split in [\"Train\", \"Validation\"]:\n",
        "        csv_path = \"{}/{}Labels.csv\".format(labels_path, split)\n",
        "        csv = pd.read_csv(csv_path)\n",
        "        C = np.array(csv['ClipID'])\n",
        "        Yt = np.array(csv.iloc[:,1:])\n",
        "        if len(Y) == 0:\n",
        "            clips = C\n",
        "            Y = Yt\n",
        "        else:\n",
        "            Y = np.concatenate((Y, Yt), axis=0)\n",
        "            clips = np.concatenate((clips, C), axis=0)\n",
        "    Y = to_categorical(np.array(Y[:,1]) // (4 // num_classes), num_classes)\n",
        "    \n",
        "\n",
        "    for i in range(len(clips)):\n",
        "        file_name = clips[i].split(\".\")[0]\n",
        "        subject = int(file_name[:6])\n",
        "        if not subject in subs:\n",
        "            subs[subject] = [0 for x in range(num_classes)]\n",
        "        subs[subject] = np.add(subs[subject], Y[i])\n",
        "\n",
        "    # total = np.sum(Y, axis=0)\n",
        "    # print(total, total / len(Y))\n",
        "    # print(subs)\n",
        "\n",
        "    folds = []\n",
        "    fold_names = []\n",
        "    for i in subs:\n",
        "        if len(folds) < n_folds:\n",
        "            folds.append(subs[i])\n",
        "            fold_names.append([i])\n",
        "        else:\n",
        "            summed = np.sum(folds, axis=1)\n",
        "            index = list(summed).index(min(summed))\n",
        "            folds[index] = np.add(folds[index], subs[i])\n",
        "            fold_names[index].append(i)\n",
        "\n",
        "    # print(folds)\n",
        "    # print(fold_names)\n",
        "    # print(np.sum(folds, axis=1))\n",
        "\n",
        "    # print(\"GETTING FOLDS DATA...\")\n",
        "    # build frames array\n",
        "    videos, labels, opface = [], [], []\n",
        "    for i in range(len(fold_names)):\n",
        "        videos.append([])\n",
        "        opface.append([])\n",
        "        labels.append([])\n",
        "\n",
        "    fps = 15\n",
        "    skip = int(round(interval * fps / time_frames, 0))\n",
        "\n",
        "    split_start = []\n",
        "    csv_path = \"{}/TrainLabels.csv\".format(labels_path)\n",
        "    csv = pd.read_csv(csv_path)\n",
        "    split_start.append(len(csv))\n",
        "    csv_path = \"{}/ValidationLabels.csv\".format(labels_path)\n",
        "    csv = pd.read_csv(csv_path)\n",
        "    split_start.append(split_start[0] + len(csv))\n",
        "\n",
        "    for Yi in range(len(clips)):\n",
        "        file_name = clips[Yi].split(\".\")[0]\n",
        "        subject = file_name[:6]\n",
        "\n",
        "        for f in range(n_folds):\n",
        "            if int(subject) in fold_names[f]:\n",
        "                # print(\"{}/{}\".format(split, subject))\n",
        "                split = 'Train' if Yi < split_start[0] else 'Validation'\n",
        "                # get aligned faces file names into files array\n",
        "                dir_path = \"{}/{}/{}/{}_aligned\".format(features_path, split, subject, file_name)\n",
        "                of_path = \"of_{}/{}/{}/{}.csv\".format(features_path, split, subject, file_name)\n",
        "                files, filesof = [], []\n",
        "\n",
        "                if os.path.isfile(of_path):\n",
        "                    csv_of = pd.read_csv(of_path)\n",
        "                    rows = csv_of[[' gaze_0_x',' gaze_0_y',' gaze_0_z',' gaze_1_x',' gaze_1_y',' gaze_1_z',' gaze_angle_x',' gaze_angle_y',' pose_Tx',' pose_Ty',' pose_Tz',' pose_Rx',' pose_Ry',' pose_Rz',' AU01_r',' AU02_r',' AU04_r',' AU05_r',' AU06_r',' AU07_r',' AU09_r',' AU10_r',' AU12_r',' AU14_r',' AU15_r',' AU17_r',' AU20_r',' AU23_r',' AU25_r',' AU26_r',' AU45_r',' AU01_c',' AU02_c',' AU04_c',' AU05_c',' AU06_c',' AU07_c',' AU09_c',' AU10_c',' AU12_c',' AU14_c',' AU15_c',' AU17_c',' AU20_c',' AU23_c',' AU25_c',' AU26_c',' AU28_c',' AU45_c']]\n",
        "                    # rows = csv_of.iloc[:,5:]\n",
        "                    min_max_scaler = MinMaxScaler()\n",
        "                    rows = min_max_scaler.fit_transform(rows)\n",
        "                \n",
        "                # there are only 150 files in each folder\n",
        "                for i in range(1,350):\n",
        "                    image_path = \"{}/frame_det_00_000{:03d}.jpg\".format(dir_path, i)\n",
        "                    if os.path.isfile(image_path) and os.path.isfile(of_path):\n",
        "                        files.append(image_path)\n",
        "                        filesof.append(rows[i-1])\n",
        "\n",
        "                # append only the parts relative to the video section\n",
        "                last = (len(files) - time_frames*skip) // skip\n",
        "                for i in range(0, last + 1, np.max([1, int(stride * time_frames)])):\n",
        "                    temp, tpof = [], []\n",
        "                    for j in range(i*skip, (i+time_frames)*skip, skip):\n",
        "                        temp.append(files[j])\n",
        "                        tpof.append(filesof[j])\n",
        "                    videos[f].append(temp)\n",
        "                    opface[f].append(tpof)\n",
        "                    labels[f].append(Y[Yi])\n",
        "\n",
        "    return videos, opface, labels\n",
        "\n",
        "\n",
        "def save_folds(folds_data):\n",
        "    videos, opface, labels = folds_data\n",
        "\n",
        "    !mkdir folds\n",
        "\n",
        "    first_only = True\n",
        "\n",
        "    # ros = RandomOverSampler(random_state=42, sampling_strategy={0: 4000, 1: 31870})\n",
        "    # rus = RandomUnderSampler(random_state=42, sampling_strategy={0: 4000, 1: 10000})\n",
        "    for i in range(n_folds):\n",
        "        print(\"SAVING FOLD {}...\".format(i+1))\n",
        "        train_X = videos.copy()\n",
        "        train_XO = opface.copy()\n",
        "        train_Y = labels.copy()\n",
        "\n",
        "        val_X = train_X.pop(i)\n",
        "        val_XO = train_XO.pop(i)\n",
        "        val_Y = train_Y.pop(i)\n",
        "\n",
        "        train_X = np.concatenate(train_X)\n",
        "        train_XO = np.concatenate(train_XO)\n",
        "        train_Y = np.concatenate(train_Y)\n",
        "\n",
        "        # create index array for resample\n",
        "        I = np.array([x for x in range(len(train_X))]).reshape(-1,1)\n",
        "\n",
        "        # I, train_Y = ros.fit_resample(I, train_Y)\n",
        "        # I, train_Y = rus.fit_resample(I, train_Y)\n",
        "        # needed only when 2 classes\n",
        "        # train_Y = to_categorical(train_Y, 2)\n",
        "\n",
        "        # copy resampled elements from original arrays according to resampling indexes\n",
        "        ntx, ntxo = [], []\n",
        "        for x in I:\n",
        "            ntx.append(train_X[x[0]])\n",
        "            ntxo.append(train_XO[x[0]])\n",
        "        train_X = ntx\n",
        "        train_XO = ntxo\n",
        "\n",
        "        # save folds to file\n",
        "        train_Y = np.array(train_Y)\n",
        "        np.save(\"folds/fold_{}_train_Y.npy\".format(i), train_Y)\n",
        "        \n",
        "        train_X = np.array(train_X)\n",
        "        np.save(\"folds/fold_{}_train_XV.npy\".format(i), train_X)\n",
        "\n",
        "        train_XO = np.array(train_XO)\n",
        "        np.save(\"folds/fold_{}_train_XO.npy\".format(i), train_XO)\n",
        "\n",
        "        val_Y = np.array(val_Y)\n",
        "        np.save(\"folds/fold_{}_validation_Y.npy\".format(i), val_Y)\n",
        "        \n",
        "        val_X = np.array(val_X)\n",
        "        np.save(\"folds/fold_{}_validation_XV.npy\".format(i), val_X)\n",
        "\n",
        "        val_XO = np.array(val_XO)\n",
        "        np.save(\"folds/fold_{}_validation_XO.npy\".format(i), val_XO)\n",
        "\n",
        "        if first_only:\n",
        "            break\n",
        "\n",
        "\n",
        "def getTest():\n",
        "    num_classes = 2\n",
        "    print(\"SAVING TEST DATA...\")\n",
        "    csv_path = \"{}/TestLabels.csv\".format(labels_path)\n",
        "    csv = pd.read_csv(csv_path)\n",
        "\n",
        "    clips = np.array(csv['ClipID'])\n",
        "    Y = to_categorical(np.array(csv['Engagement']) // (4 // num_classes), num_classes)\n",
        "\n",
        "    fps = 15\n",
        "    skip = int(round(interval * fps / time_frames, 0))\n",
        "\n",
        "    videos, opface, labels = [], [], []\n",
        "\n",
        "    for Yi in range(len(clips)):\n",
        "        file_name = clips[Yi].split(\".\")[0]\n",
        "        subject = file_name[:6]\n",
        "\n",
        "        dir_path = \"{}/Test/{}/{}_aligned\".format(features_path, subject, file_name)\n",
        "        of_path = \"of_{}/Test/{}/{}.csv\".format(features_path, subject, file_name)\n",
        "\n",
        "        files, filesof = [], []\n",
        "\n",
        "        if os.path.isfile(of_path):\n",
        "            csv_of = pd.read_csv(of_path)\n",
        "            rows = csv_of[[' gaze_0_x',' gaze_0_y',' gaze_0_z',' gaze_1_x',' gaze_1_y',' gaze_1_z',' gaze_angle_x',' gaze_angle_y',' pose_Tx',' pose_Ty',' pose_Tz',' pose_Rx',' pose_Ry',' pose_Rz',' AU01_r',' AU02_r',' AU04_r',' AU05_r',' AU06_r',' AU07_r',' AU09_r',' AU10_r',' AU12_r',' AU14_r',' AU15_r',' AU17_r',' AU20_r',' AU23_r',' AU25_r',' AU26_r',' AU45_r',' AU01_c',' AU02_c',' AU04_c',' AU05_c',' AU06_c',' AU07_c',' AU09_c',' AU10_c',' AU12_c',' AU14_c',' AU15_c',' AU17_c',' AU20_c',' AU23_c',' AU25_c',' AU26_c',' AU28_c',' AU45_c']]\n",
        "            # rows = csv_of.iloc[:,5:]\n",
        "            min_max_scaler = MinMaxScaler()\n",
        "            rows = min_max_scaler.fit_transform(rows)\n",
        "        \n",
        "        for i in range(1,350):\n",
        "            image_path = \"{}/frame_det_00_000{:03d}.jpg\".format(dir_path, i)\n",
        "            if os.path.isfile(image_path) and os.path.isfile(of_path):\n",
        "                files.append(image_path)\n",
        "                filesof.append(rows[i-1])\n",
        "\n",
        "        # append only the parts relative to the video section\n",
        "        last = (len(files) - time_frames*skip) // skip\n",
        "        for i in range(0, last + 1, np.max([1, int(stride * time_frames)])):\n",
        "            temp, tpof = [], []\n",
        "            for j in range(i*skip, (i+time_frames)*skip, skip):\n",
        "                temp.append(files[j])\n",
        "                tpof.append(filesof[j])\n",
        "            videos.append(temp)\n",
        "            opface.append(tpof)\n",
        "            labels.append(Y[Yi])\n",
        "\n",
        "    np.save(\"folds/test_XV.npy\", videos)\n",
        "    np.save(\"folds/test_XO.npy\", opface)\n",
        "    np.save(\"folds/test_Y.npy\", labels)\n",
        "\n",
        "\n",
        "class Generator_V(Sequence):\n",
        "    def __init__(self, gen_split, batch_size, frames):\n",
        "        print(\"BUILDING GENERATOR...\")\n",
        "        self.batch_size = batch_size \n",
        "        self.labels, self.videos, = [], []\n",
        "\n",
        "        file_prefix = \"\" if gen_split == \"Test\" else \"fold_{}_\".format(fold_step-1)\n",
        "        self.videos = np.load(\"folds/{}{}_XV.npy\".format(file_prefix, gen_split.lower()))\n",
        "        self.opface = np.load(\"folds/{}{}_XO.npy\".format(file_prefix, gen_split.lower()))\n",
        "        self.labels = np.load(\"folds/{}{}_Y.npy\".format(file_prefix, gen_split.lower()))\n",
        "\n",
        "        # self.videos, self.labels = shuffle(self.videos, self.labels, random_state=0)\n",
        "        print(\"Done\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.videos) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_xv = self.videos[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_xo = self.opface[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        videos, opface = [], []\n",
        "        for video in batch_xv:\n",
        "            images = []\n",
        "            for name in video:\n",
        "                img = cv2.imread(name)\n",
        "                images.append(img/255)\n",
        "            videos.append(np.array(images))\n",
        "\n",
        "        # videos = np.expand_dims(videos, axis=4)\n",
        "        videos = np.array(videos)\n",
        "        opface = np.array(batch_xo)\n",
        "        label = np.array(batch_y)\n",
        "\n",
        "        # regressor\n",
        "        # label = np.array(batch_y).argmax(axis=1) / 3\n",
        "\n",
        "        return [videos, opface], label\n",
        "        # return videos, label\n",
        "        # return opface, label\n",
        "\n",
        "\n",
        "def build_model(epoch=0, print_model=False):\n",
        "    from tcn import TCN, tcn_full_summary\n",
        "\n",
        "    img_size = 224\n",
        "    of_features = 49\n",
        "\n",
        "    def model_convlstm(input):\n",
        "        filters = 256\n",
        "\n",
        "        x = ConvLSTM2D(filters, kernel_size=3, strides=1, padding='same', kernel_regularizer=l2(5e-4), recurrent_regularizer=l2(1e-6), return_sequences=True)(input)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        x = ConvLSTM2D(filters, kernel_size=3, strides=1, padding='same', kernel_regularizer=l2(5e-4), recurrent_regularizer=l2(1e-6), return_sequences=False)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        x = MaxPooling2D(pool_size=3, strides=2, padding='valid')(x)\n",
        "        x = Flatten()(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def model_openface(input):\n",
        "        filters = 1024\n",
        "\n",
        "        x = TCN(filters)(input)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def model_inception(input):\n",
        "        inception = tf.keras.applications.InceptionResNetV2(\n",
        "            input_shape = (img_size, img_size, 3),\n",
        "            weights = 'imagenet',\n",
        "            include_top = False\n",
        "        )\n",
        "\n",
        "        for layer in inception.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        x = TimeDistributed(inception)(input)\n",
        "\n",
        "        return x\n",
        "\n",
        "    print(\"Building model...\")\n",
        "    ########### IF BUILT, MUST DEFINE A NAME TO APPEND TO DIRECTORY NAME ###############\n",
        "    global ident_name\n",
        "\n",
        "    ########### IF LOADED, MUST DEFINE DIR NAME AND STARTING EPOCH ############\n",
        "    global dir_name\n",
        "\n",
        "    if not epoch:\n",
        "        input_v = Input(shape=(time_frames, img_size, img_size, 3))\n",
        "        input_o = Input(shape=(time_frames, of_features))\n",
        "\n",
        "        m1 = model_inception(input_v)\n",
        "        m2 = model_convlstm(m1)\n",
        "        m3 = model_openface(input_o)\n",
        "\n",
        "        # joint model\n",
        "        x = Concatenate()([m2, m3])\n",
        "        \n",
        "        x = Dense(512, activation='relu')(x)\n",
        "        x = Dropout(0.8)(x)\n",
        "        x = Dense(512, activation='relu')(x)\n",
        "        x = Dropout(0.8)(x)\n",
        "        x = Dense(2, activation='softmax')(x)\n",
        "\n",
        "        model = Model([input_v, input_o], x)\n",
        "        # model = Model(input_v, x)\n",
        "\n",
        "        model.compile(\n",
        "            # loss='mse',\n",
        "            loss='categorical_crossentropy',\n",
        "            optimizer = SGD(learning_rate=0.0001, momentum=0.9), \n",
        "            # optimizer = Adam(learning_rate=1e-5),\n",
        "            # metrics=[avgacc,'accuracy'])\n",
        "            # metrics=['mse'])\n",
        "            metrics=['accuracy'])\n",
        "    \n",
        "        t = datetime.datetime.now()\n",
        "        prefix = str(t.year) +'-'+ str(t.month) +'-'+ str(t.day) +'-'+ str(t.hour) +'-'+ str(t.minute) +'-'+ str(t.second)\n",
        "        save_dir = \"{}/{}-{}\".format(drive_save_path, prefix, ident_name)\n",
        "        os.mkdir(save_dir)\n",
        "    else:\n",
        "        file_name = \"{:03d}.h5\".format(epoch)\n",
        "        save_dir = \"{}/{}\".format(drive_save_path, dir_name)\n",
        "        print(\"Loading model from {}/{}.\".format(save_dir, file_name))\n",
        "        model = load_model(\"{}/{}\".format(save_dir, file_name), custom_objects={'TCN': TCN})\n",
        "\n",
        "    if print_model:\n",
        "        plot_model(model, show_layer_names=False, show_shapes=True, expand_nested=True)\n",
        "        display(Image('model.png'))\n",
        "\n",
        "    return model, save_dir, epoch\n",
        "\n",
        "\n",
        "def set_callbacks():\n",
        "    #callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath = save_dir + '/{epoch:03d}.h5', \n",
        "        monitor = 'val_loss', \n",
        "        verbose=1, \n",
        "        save_best_only=True,\n",
        "    )\n",
        "\n",
        "    tensorboard = TensorBoard(\n",
        "    \tlog_dir         = \"{}/logs\".format(save_dir),\n",
        "    \thistogram_freq  = 0,\n",
        "    \twrite_graph     = True,\n",
        "    \twrite_grads     = False,\n",
        "    \twrite_images    = True\n",
        "    )\n",
        "\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor \t= 'val_loss',\n",
        "        patience \t= 20,\n",
        "        restore_best_weights = True,\n",
        "        verbose     = 1,\n",
        "        min_delta   = 1e-5\n",
        "    )\n",
        "\n",
        "    reduce_lr_plateau = ReduceLROnPlateau(\n",
        "        monitor \t= 'val_loss',\n",
        "        factor\t\t= 0.5,\n",
        "        patience\t= 10,\n",
        "        min_lr\t\t= 1e-6,\n",
        "        verbose     = 1\n",
        "    )\n",
        "\n",
        "    # return [checkpoint, tensorboard]\n",
        "    return [checkpoint, early_stop, reduce_lr_plateau, tensorboard]\n",
        "\n",
        "\n",
        "def fit_model(train_weights, epoch=0):\n",
        "    #calculate weights based on train set distribution\n",
        "    num_classes = len(train_weights)\n",
        "    if train_weights == [1 for x in range(num_classes)]:\n",
        "        weights = {x:1 for x in range(num_classes)}\n",
        "    else:\n",
        "        weights = {x: train_weights[x] for x in range(len(train_weights))}\n",
        "        print(\"Train weights: {}\".format(weights))\n",
        "\n",
        "    def run():\n",
        "        #run the model\n",
        "        return model.fit(\n",
        "            gen_train,\n",
        "            epochs = 1000,\n",
        "            validation_data = gen_val,\n",
        "            class_weight = weights,\n",
        "            callbacks = callbacks,\n",
        "            initial_epoch = epoch)\n",
        "        \n",
        "    hist = run()  \n",
        "\n",
        "    plot_hist(hist)\n",
        "\n",
        "    return hist\n",
        "\n",
        "\n",
        "def plot_hist(hist):\n",
        "    plt.plot(hist.history['loss'], '#0000ff', label=\"loss\")\n",
        "    plt.plot(hist.history['val_loss'], '#ff0000', label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(hist.history['accuracy'], '#0000ff', label=\"acc\")\n",
        "    plt.plot(hist.history['val_accuracy'], '#ff0000', label=\"val_acc\")\n",
        "    # plt.plot(hist.history['accuracy'], '#0055aa', label=\"acc\")\n",
        "    # plt.plot(hist.history['val_accuracy'], '#aa5500', label=\"val_acc\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def predict(**kw):\n",
        "    global gen_test\n",
        "\n",
        "    Y_true = np.load(\"folds/test_Y.npy\")\n",
        "    num_classes = Y_true.shape[1]\n",
        "    Y_true = Y_true.argmax(axis=1)\n",
        "\n",
        "    print(\"Predicting...\")\n",
        "    Y_pred = model.predict(gen_test, verbose=1).argmax(axis=1)\n",
        "    print(\"\\nConfusion matrix:\")\n",
        "    cm = confusion_matrix(Y_true, Y_pred)\n",
        "    print(cm)\n",
        "\n",
        "    print(\"\\nEvaluating...\")\n",
        "    ev = model.evaluate(gen_test, verbose=1)\n",
        "    # print(\"Loss: {}, Acc: {}\".format(ev[0], ev[1]))\n",
        "\n",
        "    hits = [0 for x in range(num_classes)]\n",
        "    total = [0 for x in range(num_classes)]\n",
        "    for i in range(len(Y_true)):\n",
        "        for c in range(num_classes):\n",
        "            if (Y_true[i] == c and Y_pred[i] == c):\n",
        "                hits[c] = hits[c] + 1\n",
        "            if Y_true[i] == c:\n",
        "                total[c] = total[c] + 1\n",
        "\n",
        "    print(\"\\nPer class accuracy:\")\n",
        "    for c in range(num_classes):\n",
        "        hits[c] = hits[c] / total[c]\n",
        "        print(\"{}: {}\".format(c, hits[c]))\n",
        "\n",
        "    print(\"Average class accuracy: {}\".format(sum(hits) / len(hits)))\n",
        "\n",
        "    f1 = f1_score(Y_true, Y_pred, average=None)\n",
        "    print(\"F1 score: {}. Avg F1: {}\".format(f1, np.mean(f1)))\n",
        "\n",
        "    return gen_test\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPw3wTPt00yn",
        "outputId": "1170a91f-6bd0-4b24-cc69-a966ef32ba8f"
      },
      "outputs": [],
      "source": [
        "# restart()\n",
        "# start_colab()\n",
        "# extract_data()\n",
        "# folds = create_folds()\n",
        "# save_folds(folds)\n",
        "# getTest()\n",
        "\n",
        "# gen_train = Generator_V('Train', batch_size, time_frames)\n",
        "# gen_val = Generator_V('Validation', batch_size, time_frames)\n",
        "# gen_test = Generator_V('Test', batch_size, time_frames)\n",
        "\n",
        "# model, save_dir, epoch = build_model(epoch, print_model=False)\n",
        "\n",
        "# callbacks = set_callbacks()\n",
        "# fit_model(list(len(gen_train.labels) / np.sum(gen_train.labels, axis=0)), epoch)\n",
        "# gen_test = predict()\n",
        "\n",
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSm_WjSKsZMZ",
        "outputId": "999b6e6e-4891-440a-dffe-08c4cdb7b2ef"
      },
      "outputs": [],
      "source": [
        "# restart()\n",
        "# start_colab()\n",
        "# extract_data()\n",
        "# folds = create_folds()\n",
        "# save_folds(folds)\n",
        "\n",
        "# gen_train = Generator_V('Train', batch_size, time_frames)\n",
        "# gen_val = Generator_V('Validation', batch_size, time_frames)\n",
        "# gen_test = Generator_V('Test', batch_size, time_frames)\n",
        "\n",
        "# model, save_dir, epoch = build_model(epoch, print_model=True)\n",
        "\n",
        "callbacks = set_callbacks()\n",
        "fit_model(list(len(gen_train.labels) / np.sum(gen_train.labels, axis=0)), epoch)\n",
        "gen_test = predict()\n",
        "\n",
        "# model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOmaVttur5olTgU+5Bdwxma",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
