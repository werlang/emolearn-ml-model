{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/werlang/emolearn-ml-model/blob/main/conv3d_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoMicotOZ_pH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from keras.engine import  Model\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import TimeDistributed, GRU, LSTM, Dropout, Conv2D, Conv3D, BatchNormalization, MaxPooling2D, MaxPooling3D, Flatten, Dense, Input\n",
        "from keras.utils import Sequence, plot_model, to_categorical\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, Callback, ModelCheckpoint\n",
        "from keras.metrics import AUC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, cv2\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
        "from keras import backend as K\n",
        "import math\n",
        "from IPython.display import Image\n",
        "\n",
        "def start_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    !pip install Keras-Applications\n",
        "\n",
        "\n",
        "def extract_data():\n",
        "    !mkdir features\n",
        "\n",
        "    # aligned faces extracted from openface\n",
        "    print(\"COPYING TRAIN SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/new_features_daisee/Train.zip\" -d features \n",
        "    print(\"COPYING TEST SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/new_features_daisee/Test.zip\" -d features \n",
        "    print(\"COPYING VALIDATION SET...\")\n",
        "    !unzip -n -q \"drive/My Drive/Doutorado/Implementação/new_features_daisee/Validation.zip\" -d features \n",
        "    print(\"COPYING LABELS...\")\n",
        "    !cp -r \"drive/My Drive/Doutorado/Implementação/new_features_daisee/labels\" ./\n",
        "    print(\"DONE\")\n",
        "\n",
        "\n",
        "class Generator_V(Sequence):\n",
        "    def __init__(self, split, batch_size, frames, **kw):\n",
        "        self.batch_size = batch_size \n",
        "        self.split = split\n",
        "\n",
        "        Y = np.load(\"{}/{}.npy\".format(labels_path, split))\n",
        "        Y_gen = to_categorical(Y, 2)\n",
        "\n",
        "        self.labels, self.videos = [], []\n",
        "        interval = 2\n",
        "        stride = 0.5\n",
        "        fps = 15\n",
        "        skip = int(round(interval * fps / frames, 0))\n",
        "\n",
        "        for Yi in range(len(Y_gen)):\n",
        "            dir_path = \"{}/{}/{}\".format(features_path, split, Yi)\n",
        "            files = []\n",
        "            for r, d, f in os.walk(dir_path):\n",
        "                for i in f:\n",
        "                    files.append(\"{}/{}\".format(r, i))\n",
        "\n",
        "            for i in range(0, (len(files) - frames*skip) // skip + 1, np.max([1, int(stride * frames)])):\n",
        "                temp = []\n",
        "                for j in range(i*skip, (i+frames)*skip, skip):\n",
        "                    temp.append(files[j])\n",
        "                self.videos.append(temp)\n",
        "                self.labels.append(Y_gen[Yi])\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.videos) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.videos[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_y = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "\n",
        "        videos = []\n",
        "        for video in batch_x:\n",
        "            images = []\n",
        "            for name in video:\n",
        "                images.append(cv2.imread(name))\n",
        "            videos.append(np.array(images)/255)\n",
        "\n",
        "        videos = np.array(videos)\n",
        "        label = np.array(batch_y)\n",
        "\n",
        "        return videos, label\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    ########### IF BUILT, MUST DEFINE A NAME TO APPEND TO DIRECTORY NAME ###############\n",
        "    ident_name = 'conv3d-1st-try'\n",
        "\n",
        "    ########### IF LOADED, MUST DEFINE DIR NAME AND STARTING EPOCH ############\n",
        "    dir_name = '2020-7-16-21-51-7-fine-tune-images-vgg-gru-striding-2'\n",
        "\n",
        "    input = Input(shape=(time_frames,224,224,3))\n",
        "    x = Conv3D(4, kernel_size=(3, 3, 3), activation='relu')(input)\n",
        "    x = Conv3D(8, kernel_size=(3, 3, 3), activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "    x = Conv3D(16, kernel_size=(3, 3, 3), padding=\"same\", activation='relu')(x)\n",
        "    x = Conv3D(32, kernel_size=(3, 3, 3), padding=\"same\", activation='relu')(x)\n",
        "    x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Dense(16, activation='relu')(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Dense(2, activation='softmax')(x)\n",
        "    model = Model(input, x)\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        # optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True), \n",
        "        optimizer = Adam(lr=1e-4),\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "    #showing the model\n",
        "\n",
        "    # print(\"GRU:\")\n",
        "    # for layer in model.layers:\n",
        "    #     print(\"{} {}\".format(layer.output.shape.as_list(), layer.trainable))\n",
        "\n",
        "    # model.summary()\n",
        "\n",
        "    plot_model(model, show_layer_names=False, show_shapes=True)\n",
        "\n",
        "    loadModel = False if epoch == 0 else True\n",
        "\n",
        "    if not loadModel:\n",
        "        t = datetime.datetime.now()\n",
        "        prefix = str(t.year) +'-'+ str(t.month) +'-'+ str(t.day) +'-'+ str(t.hour) +'-'+ str(t.minute) +'-'+ str(t.second)\n",
        "        save_dir = \"{}/{}-{}\".format(drive_save_path, prefix, ident_name)\n",
        "        os.mkdir(save_dir)\n",
        "    else:\n",
        "        file_name = \"{:03d}.h5\".format(epoch)\n",
        "        save_dir = \"{}/{}\".format(drive_save_path, dir_name)\n",
        "        print(\"Loading model from {}/{}.\".format(save_dir, file_name))\n",
        "        model = load_model(\"{}/{}\".format(save_dir, file_name))\n",
        "\n",
        "    return model, save_dir, epoch\n",
        "\n",
        "\n",
        "def set_callbacks():\n",
        "    #callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath = save_dir + '/{epoch:03d}.h5', \n",
        "        monitor = 'val_loss', \n",
        "        verbose=1, \n",
        "        save_best_only=True,\n",
        "    )\n",
        "\n",
        "    tensorboard = TensorBoard(\n",
        "    \tlog_dir         = \"{}/logs\".format(save_dir),\n",
        "    \thistogram_freq  = 0,\n",
        "    \twrite_graph     = True,\n",
        "    \twrite_grads     = False,\n",
        "    \twrite_images    = True\n",
        "    )\n",
        "\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor \t= 'val_loss',\n",
        "        patience \t= 10,\n",
        "        restore_best_weights = True,\n",
        "        verbose     = 1,\n",
        "        min_delta   = 1e-5\n",
        "    )\n",
        "\n",
        "    reduce_lr_plateau = ReduceLROnPlateau(\n",
        "        monitor \t= 'val_loss',\n",
        "        factor\t\t= 0.2,\n",
        "        patience\t= 5,\n",
        "        min_lr\t\t= 1e-8,\n",
        "        verbose     = 1\n",
        "    )\n",
        "\n",
        "    return [checkpoint, early_stop, reduce_lr_plateau]\n",
        "\n",
        "\n",
        "def build_generators():\n",
        "    #build the data generators\n",
        "    print(\"Building generators...\")\n",
        "    gen_train = Generator_V('Train', batch_size, time_frames)\n",
        "    gen_val = Generator_V('Validation', batch_size, time_frames)\n",
        "    print(\"DONE\")\n",
        "\n",
        "    return gen_train, gen_val\n",
        "\n",
        "\n",
        "def fit_model():\n",
        "    #calculate weights based on train set distribution\n",
        "    label_array = np.load(\"{}/Train.npy\".format(labels_path))\n",
        "    class_ratio = np.sum(label_array) / len(label_array)\n",
        "    weights = {0: class_ratio, 1: 1 - class_ratio}\n",
        "\n",
        "    def run():\n",
        "        #run the model\n",
        "        model.fit_generator(\n",
        "            gen_train,\n",
        "            epochs = 1000,\n",
        "            validation_data = gen_val,\n",
        "            class_weight = weights,\n",
        "            callbacks = callbacks,\n",
        "            initial_epoch = epoch)\n",
        "        \n",
        "    run()  \n",
        "\n",
        "\n",
        "def predict():\n",
        "    print(\"Preparing data...\")\n",
        "    gen_test = Generator_V('Test', batch_size, time_frames)\n",
        "\n",
        "    def get_Y_true():\n",
        "        Y = np.load(\"{}/Test.npy\".format(labels_path))\n",
        "\n",
        "        labels = []\n",
        "        vtime = 10\n",
        "        interval = 2\n",
        "        stride = 0.5\n",
        "        n_videos = int((vtime / interval - 1) / stride + 1)\n",
        "        \n",
        "        for Yi in range(len(Y)):    \n",
        "            for i in range(n_videos):\n",
        "                labels.append(Y[Yi])\n",
        "        \n",
        "        return np.array(labels)\n",
        "\n",
        "    Y_true = get_Y_true()\n",
        "   \n",
        "    print(\"Predicting...\")\n",
        "    Y_pred = (model.predict(gen_test, verbose=1)).argmax(axis=1)\n",
        "    print(\"Confusion matrix:\\n\")\n",
        "    cm = confusion_matrix(Y_true, Y_pred)\n",
        "    print(cm)\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    ev = model.evaluate(gen_test, verbose=1)\n",
        "    print(\"\\nLoss: {}, Acc: {}\".format(ev[0], ev[1]))\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "51ktnwPk3qjT",
        "outputId": "631578f2-89a3-46bd-bf3f-80ccf69b82de",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "labels_path = \"labels\"\n",
        "features_path = \"features\"\n",
        "drive_save_path = 'drive/My Drive/1NOSYNC/DT/checkpoint'\n",
        "batch_size = 20\n",
        "labels = ['Test', 'Train', 'Validation']\n",
        "time_frames = 10\n",
        "epoch = 0\n",
        "\n",
        "# start_colab()\n",
        "# extract_data()\n",
        "\n",
        "# gen_train, gen_val = build_generators()\n",
        "model, save_dir, epoch = build_model()\n",
        "callbacks = set_callbacks()\n",
        "fit_model()\n",
        "# predict()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
